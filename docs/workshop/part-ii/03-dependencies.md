---
description: Extracting shared dependencies across multiple requests
title: Extracting shared dependencies
id: shared-dependencies
slug: ../shared-dependencies
---
```mdx-code-block
import useBaseUrl from '@docusaurus/useBaseUrl';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import PartialVerifyKratixWithOutPromises from '../../_partials/workshop/_verify-kratix-without-promises.md';
```

This is Part 2 of [a series](intro) illustrating how Kratix works. <br />
üëàüèæ&nbsp;&nbsp; Previous: [Delivering your service on demand](service-on-demand) <br />
üëâüèæ&nbsp;&nbsp; Next: [Intentionally schedule Promise resources](schedule-promise)

<hr />

**In this tutorial, you will**
* [Understanding Kratix Promise dependencies](#understanding-dependencies)
* [Splitting out Elastic Cloud Kubernetes (ECK) dependencies](#splitting-dependencies)
* [Install the Promise with separate dependencies](#install-promise)
* [Make multiple Resource Requests ](#resource-requests)
* [Summary](#summary)
* [Clean up environment](#cleanup)

## Understanding Kratix Promise dependencies {#understanding-dependencies}

## Splitting out Elastic Cloud Kubernetes (ECK) dependencies {#splitting-dependencies}

After the previous tutorial step, the ECK Promise bundled all necessary provisioning
steps into the Promise pipeline. This made it possible to only do a single Resource Request
because of duplication across the requests.

The pipeline `run` script followed the installation instructions in the ECK
documentation [here](https://www.elastic.co/guide/en/cloud-on-k8s/2.8/k8s-deploy-eck.html).
Namely, the two separate files downloaded, the CRDs and the operator. Then creating
the required instances of the CRDs.

The operator and CRDs are a set of resources that only need to (and can be)
installed once in a cluster. After they are installed they can serve as many
requests as needed. Therefore, if you want to support provisioning multiple
resource requests you will need to change how the CRDs and operator resources are installed
into the cluster so that there is not a conflict.

<img src={useBaseUrl('/img/docs/workshop/operator-as-shared-dependency.png')} />

### Worker Cluster Resources

Currently, both the operator and the request for an instance from the operator are generated in the pipeline.
Kratix has the concept of `workerClusterResources` which can be useful to manage different
types of resources. While a pipeline runs on every request for a resource, the Worker Cluster
Resources are a set of resources that only need to be installed once per cluster for the given Promise.

A simple use cases may be to create a shared namespace that all subsequent
resource requests send there output to. In the case of this ECK Promise, you can
install the Operator and CRDs as a Worker Cluster Resources.

### Remove shared dependencies from the pipeline

The following steps will refactor this Promise to instead separate shared dependencies from individual request resources:

<img src={useBaseUrl('/img/docs/workshop/promise-with-dependencies.png')} />

#### Remove one-off files (i.e. dependencies) from pipeline

First you will need to remove the Operator and CRD files from the pipeline `run` script in order in order
to stop them from being created on all Resource Requests.

Open the `pipeline/run` file and remove lines 9-16. This will remove both curl commands which download the CRDs and controller resources.

#### Run the test suite, see it passing

With the downloads removed, you can re-run the test suite and see that the resulting files no long include
the Operator or the CRDs.
```bash
scripts/test-pipeline
```

Once the execution completes, use the following command to check the files generated by the pipeline:

```bash
tree test
```

Verify that the output shows only the following files:

```shell-session
üìÇ test
‚îú‚îÄ‚îÄ input
‚îÇ   ‚îî‚îÄ‚îÄ object.yaml
‚îú‚îÄ‚îÄ metadata
‚îî‚îÄ‚îÄ output
    ‚îú‚îÄ‚îÄ beats.yaml
    ‚îú‚îÄ‚îÄ elasticsearch.yaml
    ‚îî‚îÄ‚îÄ kibana.yaml
```

### Add shared dependencies as Worker Cluster Resources

Removing the files from the pipeline is not enough. You must now also add them to the Promise as Worker Cluster Resources.

#### Download the WorkerResourcesBuilder

Run the following command to create a `resources` directory where you can store these files and any others that you may want to depend on for the Promise installation:
```bash
mkdir -p resources

curl --silent --location --output resources/elastic-crds.yaml https://download.elastic.co/downloads/eck/2.8.0/crds.yaml
curl --silent --location --output resources/elastic-operator.yaml https://download.elastic.co/downloads/eck/2.8.0/operator.yaml
```

Once stored locally, you will need to add these resources to the Promise file. The resources are added
as a list under `workerClusterResources` which can tricky with formatting and require some subtle white
space changes.

To make this step simpler there is a _very basic_ tool which grabs all YAML
documents from a single directory and injects them correctly into the `workerClusterResources`
field in the `promise.yaml`.

To use this tool, you will need to download the correct binary for your computer
from [GitHub releases](https://github.com/syntasso/kratix/releases/tag/v0.0.4):

:::info

If you are using Instruqt, this binary has already been made available to you.
All you need is:

```bash
mkdir -p bin
cp /root/bin/worker-resource-builder ./bin
```

:::

<Tabs>
  <TabItem value="darwin-amd64" label="Intel Mac" default>

    mkdir -p bin
    curl -sLo ./bin/worker-resource-builder.tar.gz https://github.com/syntasso/kratix/releases/download/v0.0.4/worker-resource-builder_0.0.4_darwin_amd64.tar.gz
    tar -xvf ./bin/worker-resource-builder.tar.gz -C ./bin
    mv ./bin/worker-resource-builder-v* ./bin/worker-resource-builder
    chmod +x ./bin/worker-resource-builder

  </TabItem>
  <TabItem value="darwin-arm64" label="Apple Silicon Mac">

    mkdir -p bin
    curl -sLo ./bin/worker-resource-builder.tar.gz https://github.com/syntasso/kratix/releases/download/v0.0.4/worker-resource-builder_0.0.4_darwin_arm64.tar.gz
    tar -xvf ./bin/worker-resource-builder.tar.gz -C ./bin
    mv ./bin/worker-resource-builder-v* ./bin/worker-resource-builder
    chmod +x ./bin/worker-resource-builder

  </TabItem>
  <TabItem value="linux-arm64" label="Linux ARM64">

    mkdir -p bin
    curl -sLo ./bin/worker-resource-builder.tar.gz https://github.com/syntasso/kratix/releases/download/v0.0.4/worker-resource-builder_0.0.4_linux_arm64.tar.gz
    tar -xvf ./bin/worker-resource-builder.tar.gz -C ./bin
    mv ./bin/worker-resource-builder-v* ./bin/worker-resource-builder
    chmod +x ./bin/worker-resource-builder

  </TabItem>
    <TabItem value="linux-amd64" label="Linux AMD64">

    mkdir -p bin
    curl -sLo ./bin/worker-resource-builder.tar.gz https://github.com/syntasso/kratix/releases/download/v0.0.4/worker-resource-builder_0.0.4_linux_amd64.tar.gz
    tar -xvf ./bin/worker-resource-builder.tar.gz -C ./bin
    mv ./bin/worker-resource-builder-v* ./bin/worker-resource-builder
    chmod +x ./bin/worker-resource-builder

  </TabItem>
</Tabs>

Once installed, you can see how to use the binary by running the following help command:
```bash
./bin/worker-resource-builder --help
```

The above command will give an output similar to:
```shell-session
Usage of ./bin/worker-resource-builder:
  -resources-dir string
        Absolute Path of k8s resources to build workerClusterResources from
  -promise string
        Absolute path of Promise to insert workerClusterResources into
```

Given this usage instructions, you can run the following command to overwrite the current Promise file to include the CRD and controller resources:

```bash
echo "current promise length is: $(wc -l promise.yaml)"
./bin/worker-resource-builder -resources-dir ./resources -promise promise.yaml | tee tmp-promise.yaml  >/dev/null; mv tmp-promise.yaml promise.yaml
echo "new promise length is: $(wc -l promise.yaml)"
```


The above command will give an output similar to:
```shell-session
current promise length is: 35 promise.yaml
new promise length is: 11398 promise.yaml
```

In this output, you can see that the the files in the `resources` directory have now been added to the `promise.yaml` file. You can also check the top of the newly edited `promise.yaml` and see that these resources have been added as list items under the `workerClusterResources` key.

:::info

You may notice that the length of the files in `resources` is shorter than what was added to the `promise.yaml` file. This is because the `worker-resources-builder` binary reformatted long lines into more readable lines with a max length of 90.

If you have [yq](https://mikefarah.gitbook.io/yq/) installed you can verify the total number of documents in both matches with the following command:
```bash
diff <(yq ea '[.] | length' resources/*) <(yq '.spec.workerClusterResources | length' promise.yaml)
```

No difference in number of YAML resources will result in no output.
:::

## Install the Promise {#install-promise}

### Prerequisite setup

<PartialVerifyKratixWithOutPromises />

### Install the Promise

With the pipeline image available, you can now install the updated Promise:

```bash
kubectl --context ${PLATFORM} create --filename promise.yaml
```

To validate the Promise has been installed, you can list all Promises by running:
```bash
kubectl --context kind-platform get promises
```

Your output will show the `elastic-cloud` Promise:
```shell-session
NAME            AGE
elastic-cloud   10s
```

In addition, you can now verify that the dependencies have been installed on the worker cluster:

```bash
kubectl --context ${WORKER} get crds | grep elastic
kubectl --context ${WORKER} get pods -n elastic-system
```

The above command will give an output similar to:
```shell-session
agents.agent.k8s.elastic.co                            2023-02-01T12:00:00Z
apmservers.apm.k8s.elastic.co                          2023-02-01T12:00:00Z
beats.beat.k8s.elastic.co                              2023-02-01T12:00:00Z
elasticmapsservers.maps.k8s.elastic.co                 2023-02-01T12:00:00Z
elasticsearchautoscalers.autoscaling.k8s.elastic.co    2023-02-01T12:00:00Z
elasticsearches.elasticsearch.k8s.elastic.co           2023-02-01T12:00:00Z
enterprisesearches.enterprisesearch.k8s.elastic.co     2023-02-01T12:00:00Z
kibanas.kibana.k8s.elastic.co                          2023-02-01T12:00:00Z
logstashes.logstash.k8s.elastic.co                     2023-02-01T12:00:00Z
stackconfigpolicies.stackconfigpolicy.k8s.elastic.co   2023-02-01T12:00:00Z

NAME                 READY   STATUS    RESTARTS   AGE
elastic-operator-0   1/1     Running   0          1m
```

## Make multiple Resource Requests {#resource-requests}

Now that you have installed the operator and CRDs as a part of Promise installation,
you can once again don the Application Engineer hat and return to the original goal of
making more than one Resource Request to the single ECK Promise.

Just as you did in the last step, you will need to make two Resource Requests with
two different resource names:

```bash
kubectl --context $PLATFORM apply --filename resource-request.yaml
cat resource-request.yaml | \
  sed 's/name: example/name: second-request/' | \
  kubectl --context $PLATFORM apply --filename -
```

With these two requests made, you can see both pipelines running simultaneously:

```bash
kubectl --context $PLATFORM get pods
```

The above command will give an output similar to:
```shell-session
NAME                                           READY   STATUS      RESTARTS   AGE
request-pipeline-elastic-cloud-default-01650   0/1     Completed   0          106s
request-pipeline-elastic-cloud-default-99684   0/1     Completed   0          11s
```

And once completed you will be able to watch for two sets of ECK resources being deployed to the Worker cluster:
```bash
kubectl --context $WORKER get pods --watch
```

Once you see all 6 pods in the output similar to below, you can use <kbd>Ctrl</kbd>+<kbd>C</kbd> to exit the watch mode:
```shell-session
NAME                                 READY   STATUS    RESTARTS   AGE
example-es-default-0                 1/1     Running   0          2m21s
example-kb-d97b489b-9twhq            1/1     Running   0          2m21s
second-request-es-default-0          1/1     Running   0          42s
second-request-kb-6cdc9594ff-7dnnm   1/1     Running   0          42s
```

## Summary {#summary}

And with that, you have reduced duplication by delivering shared dependencies separately from the on-demand service! While this workshop only showcases two instances both deployed to the same cluster, this architecture can easily be used to support an unlimited number of instances across an unlimited number of clusters.

To recap the steps you took:
1. ‚úÖ&nbsp;&nbsp;Evaluated what resources are shared dependencies
1. ‚úÖ&nbsp;&nbsp;Moved any shared dependencies from pipeline resources to worker cluster resources
1. ‚úÖ&nbsp;&nbsp;Viewed the dependency set up on Promise install
1. ‚úÖ&nbsp;&nbsp;Successfully request more than one instance of ECK

## Clean up environment {#cleanup}

Before moving on, please remove the ECK Promise from your cluster.

To delete all the Promises:
```bash
kubectl --context $PLATFORM delete promises --all
```

## üéâ &nbsp; Congratulations!
‚úÖ&nbsp;&nbsp;Your Promise can deliver on-demand services that have shared dependencies.<br />
üëâüèæ&nbsp;&nbsp;Next you will [Intentionally schedule Promise resources](schedule-promise).
